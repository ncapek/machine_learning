{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "Motto of this and all subsequent courses: **\"If you can't implement it, you don't understand it.\"**\n",
    "\n",
    "The very first machine learning model which serves as an introduction to the field.\n",
    "\n",
    "## Statistics vs Machine learning\n",
    "- Machine learning approach - interested in models with high predictive accuracy, loss functions, minimizing loss functions, train and test sets, hyperparameters, generalization, overfitting, regularization\n",
    "\n",
    "\n",
    "- Statistics approach - interested in models with strong explanatory power, significance tests (ANOVA, t-tests etc.), model \n",
    "diagnostics, model building and selection (forward, backward stepwise), evaluation (AIC, BIC), standard transformations (Box-Cox), residual analysis\n",
    "\n",
    "## Supervised learning vs unsupervised learning\n",
    "- Supervised learning has a target variable, which we want to learn how to predict. Linear regression is in this category\n",
    "    - Regression - trying to predict a real valued number, eg. temperature, price, age\n",
    "    - Classification - trying to predict a category, eg. day of month, blood type, presence of cancer\n",
    "- Unsupervised learning does not have a target variable and the goal is to reveals underlying patterns within the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1D Linear regression\n",
    "- Problem definition: Given some data, we want to find a line that best fits that data, so we can make predictions from input variables alone\n",
    "\n",
    "<img src=\"assets/1dregression.png\"\n",
    "     alt=\"Markdown Monster icon\"\n",
    "     style=\"float: left; margin-right: 10px\"\n",
    "     width=\"300\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries\n",
    "\n",
    "- given a set of datapoints $data = \\{(x_1, y_1), ..., (x_N, y_N)\\}$, where $(x_i, y_i)$ denote the pair of input variable $x$ and target variable $y$ for datapoint $i$, $N$ denotes the number of datapoints\n",
    "- a 1D line is given by the equation $y = mx + b$,\n",
    "- for a fitted line $\\hat{y}$ and point $(x_i, y_i)$, the prediction value is given by $\\hat{y_i} = ax_i + b$\n",
    "- define a cost function $J$ to quantify how well the data is being fit\n",
    "- $J = \\sum_{i = 1}^{N} (\\hat{y_i} - y_i)^2$, also known as the sum of square errors, ie. for every datapoint find the difference between the predicted value and the actual value and square it\n",
    "- defining $J$ as such is a good choice, since the difference between predicted values and actual values will always be positive. If we didn't square the difference, the errors would end up cancelling out. Furthermore larger error get penalized more due to the square."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution\n",
    "What follows is an approach to finding the parameters $a, b$ such that the data is best fit, ie. the cost function is minimized\n",
    "\n",
    "### Calculus essentials\n",
    "- from calculus we know that the gradient is a vector of partial derivatives with respect to the inputs\n",
    "- $\\triangledown f(p) = \\begin{bmatrix} \\displaystyle \\frac{\\partial f(p)}{\\partial x_1} \\\\ \\vdots \\\\ \\displaystyle \\frac{\\partial f(p)}{\\partial x_n} \\end{bmatrix}$, given a function $f$ at point $p$ and $n$ input variables, $\\triangledown f(p)$ gives the gradient at point $p$\n",
    "\n",
    "**Statement:** *The gradient points to the direction of steepest ascent*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/gradient.png\"\n",
    "     alt=\"Markdown Monster icon\"\n",
    "     style=\"float: left; margin-right: 10px\"\n",
    "     width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- for a given input value $w_0$, consider its gradients\n",
    "- for the subset of $w$ values yielding a positive gradient, by subtracting the gradient from $w_0$, we move to the left where, correspondingly, the value of $J(w)$ decreases\n",
    "- likewise for the subsect of $w$ values yielding a negative gradient, by subtracting the gradient from $w_0$, we move to the right (subtracting a negative is equivalent to adding), we move to the right where, correspondingly, the value of $J(w)$ decreases\n",
    "\n",
    "#### Gradient descent\n",
    "Thus we arrive at the crucial algorithm used to minimize cost functions in machine learning, **the gradient descent algorithm**\n",
    "\n",
    "- (general case for multiple input variables) for every input parameter $x_i$ we update its value by subtracting a small multiple $\\epsilon$ (epsilon)  of the partial derivative with respect to $x_i$\n",
    "- $x_i := x_i - \\frac{\\partial J(x_i)}{\\partial x_i}$\n",
    "- for a more compact notation we place all input variables into a vector $\\theta$ \n",
    "- $\\theta = \\begin{bmatrix} x_1 \\\\ \\vdots \\\\ x_n \\end{bmatrix}$\n",
    "- the we can express the simultaneous update of all input variables in a single equation\n",
    "- $\\theta = \\theta - \\epsilon \\triangledown{J}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Closed form solution\n",
    "- though we could certainly find the optimal parameters for linear regression with the use of gradient descent, linear regression is a special case of machine learning algorithm for which a closed form solution exists\n",
    "- a closed form solution indicates that the optimal parameters can be gained from a direct calculation\n",
    "- gradient descent gets us into a minimum by continuously going \"downhill\" on the cost function until we reach a point where the gradient is equal to zero\n",
    "- the gradient could also however be equal to zero in a maximum, so with a closed form solution it might not be clear if a minimum is found, **EXCEPT** that the Squared error function is a *convex*, for which we know a global minimum will be found\n",
    "\n",
    "Once again: we would like to find a line $\\hat{y} = ax + b$, given by parameters $a, b$. We have also defined a cost function $J$, which quantifies how well $\\hat{y}$ fits the data. The best fit will be given by values $a, b$ which minimize $J$. These values can be found by figuring out where $\\triangledown{J} = \\vec{0}$, ie. $\\frac{\\partial J}{\\partial a} = 0$ and $\\frac{\\partial J}{\\partial b} = 0$\n",
    "\n",
    "$J = \\sum_{i = 1}^{N} (\\hat{y_i}- y_i )^2$  \n",
    "$\\equiv J = \\sum_{i = 1}^{N} (y_i - \\hat{y_i})^2$  \n",
    "$ \\equiv J = \\sum_{i = 1}^{N} (y_i - (ax_i + b))^2$\n",
    "\n",
    "___\n",
    "\n",
    "$\\frac{\\partial J}{\\partial a} = \\frac{\\partial}{\\partial a}\\sum_{i = 1}^{N} (y_i - (ax_i + b))^2 = 0$  \n",
    "$\\equiv \\frac{\\partial}{\\partial a}(y_1 - (ax_1 + b))^2 + \\dots + \\frac{\\partial}{\\partial a}(y_N - (ax_N + b))^2 = 0$   \n",
    "$\\equiv \\frac{\\partial}{\\partial a}(y_1 - ax_1 - b)^2 + \\dots + \\frac{\\partial}{\\partial a}(y_N - ax_N - b)^2 = 0$  \n",
    "$\\equiv 2(y_1 - ax_1 - b)(-x_1) + \\dots + 2(y_N - ax_N - b)(-x_N) = 0$  \n",
    "$\\equiv -2(y_1x_1 - ax_1^2 - bx_1) - \\dots - 2(y_Nx_N - ax_N^2 - bx_N) = 0$  \n",
    "$\\equiv (y_1x_1 - ax_1^2 - bx_1) + \\dots + (y_Nx_N - ax_N^2 - bx_N) = 0$  \n",
    "$\\equiv (y_1x_1 + \\dots \\ y_Nx_N) = (ax_1^2 + \\dots + ax_N^2) + (bx_1 + \\dots + bx_N)$  \n",
    "$\\equiv (y_1x_1 + \\dots \\ y_Nx_N) = a(x_1^2 + \\dots + x_N^2) + b(x_1 + \\dots + x_N)$  \n",
    "$\\equiv \\sum_{i=1}^{N} y_ix_i = a\\sum_{i=1}^{N} x_i^2 + b\\sum_{i=1}^{N} x_i$ &ensp;&ensp; $(1)$\n",
    "\n",
    "___\n",
    "\n",
    "$\\frac{\\partial J}{\\partial b} = \\frac{\\partial}{\\partial b}\\sum_{i = 1}^{N} (y_i - (ax_i + b))^2 = 0$  \n",
    "$\\equiv \\frac{\\partial}{\\partial b}(y_1 - (ax_1 + b))^2 + \\dots + \\frac{\\partial}{\\partial b}(y_N - (ax_N + b))^2 = 0$    \n",
    "$\\equiv \\frac{\\partial}{\\partial b}(y_1 - ax_1 - b)^2 + \\dots + \\frac{\\partial}{\\partial b}(y_N - ax_N - b)^2 = 0$  \n",
    "$\\equiv 2(y_1 - ax_1 - b)(-1) + \\dots + 2(y_N - ax_N - b)(-1) = 0$  \n",
    "$\\equiv (y_1 - ax_1 - b) + \\dots + (y_N - ax_N - b) = 0$  \n",
    "$\\equiv (y_1 + \\dots + y_N) = (ax_1 + \\dots + ax_N) + (b + \\dots + b)$  \n",
    "$\\equiv (y_1 + \\dots + y_N) = a(x_1 + \\dots + x_N) + bN$  \n",
    "$\\equiv \\sum_{i=1}^{N}y_i = a\\sum_{i=1}^{N}x_i + bN$ &ensp;&ensp; $(2)$\n",
    "\n",
    "Substituting into $(1)$ and $(2)$:\n",
    "- $C = \\sum_{i=1}^{N} x_i^2$  \n",
    "- $D = \\sum_{i=1}^{N} x_i$  \n",
    "- $E = \\sum_{i=1}^{N} y_ix_i$  \n",
    "- $F = \\sum_{i=1}^{N}y_i$   \n",
    "\n",
    "yields:  \n",
    "$E = aC + bD$  \n",
    "$F = aD + bN$\n",
    "\n",
    "solving for $a, b$:  \n",
    "$a = \\frac{NE - DF}{NC - D^2}$  \n",
    "$b = \\frac{FC - DE}{NC - D^2}$\n",
    "\n",
    "and subtituting back:  \n",
    "$a = \\frac{N\\sum_{i=1}^{N} y_ix_i - \\sum_{i=1}^{N} x_i \\sum_{i=1}^{N}y_i}{N \\sum_{i=1}^{N} x_i^2 - {(\\sum_{i=1}^{N} x_i)}^2}$  \n",
    "$b = \\frac{\\sum_{i=1}^{N}y_i\\sum_{i=1}^{N} x_i^2 - \\sum_{i=1}^{N} x_i \\sum_{i=1}^{N} y_ix_i}{N\\sum_{i=1}^{N} x_i^2 - {(\\sum_{i=1}^{N} x_i)}^2}$\n",
    "\n",
    "And we're done. Parameters $a, b$ can now be calculated directly from the datapoints.\n",
    "But for further simplification we divide the numerators and denominators by $N^2$  \n",
    "$a = \\frac{\\frac{\\sum_{i=1}^{N} y_ix_i}{N} - \\frac{\\sum_{i=1}^{N} x_i}{N} \\frac{\\sum_{i=1}^{N}y_i}{N}}{\\frac{\\sum_{i=1}^{N} x_i^2}{N} - {(\\frac{\\sum_{i=1}^{N} x_i}{N})}^2}$  \n",
    "$b = \\frac{\\frac{\\sum_{i=1}^{N}y_i}{N} \\frac{\\sum_{i=1}^{N} x_i^2}{N} - \\frac{\\sum_{i=1}^{N} x_i}{N} \\frac{\\sum_{i=1}^{N} y_ix_i}{N}}{\\frac{\\sum_{i=1}^{N} x_i^2}{N} - {(\\frac{\\sum_{i=1}^{N} x_i}{N})}^2}$\n",
    "\n",
    "Now using the definition of the mean:  \n",
    "$\\bar{x} = \\frac{\\sum_{i=1}^{N}x_i}{N}$  \n",
    "$\\bar{xy} = \\frac{\\sum_{i=1}^{N}x_iy_i}{N}$\n",
    "\n",
    "Subtitute into $a, b$:  \n",
    "$a = \\frac{\\bar{xy} - \\bar{x}\\bar{y}}{\\bar{x^2} - \\bar{x}^2}$  \n",
    "$b = \\frac{\\bar{y} \\bar{x^2} - \\bar{x} \\bar{xy}}{\\bar{x^2} - \\bar{x}^2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding 1D linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a = 2.0817640146785092, b = 0.02242686467597558\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO3dd5SU5fnG8e8dyk9AFMgiWRFYJGDBhqyoIVESMaKxGwkmAioJNqQIGgIoRAERsYBEpSoWFKKgqFgAC2o8KKuoKEqvIoiAgEbq8/vjGczC7rpT3invzPU5h8Pu7My89x7l2mfv9ynmnENERMLnZ+kuQERE4qMAFxEJKQW4iEhIKcBFREJKAS4iElIVU3mxvLw8V1BQkMpLioiEXlFR0QbnXO39H09pgBcUFDB37txUXlJEJPTMbEVpj6uFIiISUuUGuJnVM7PXzWyBmX1qZt0ijw8wszVmNi/y55zklysiIntF00LZBfR0zn1gZtWBIjObEfnavc65YckrT0REylJugDvn1gJrIx9vNbMFQN1kFyYiIj8tph64mRUAzYA5kYe6mNnHZjbezGqW8ZrOZjbXzOZ+/fXXCRUrIiL/E3WAm9mBwDNAd+fcFuBBoBFwAn6Efndpr3POjXbOFTrnCmvXLjELRkRE4hRVgJtZJXx4P+GcmwLgnFvnnNvtnNsDjAFaJK9MERHZXzSzUAwYByxwzt1T7PH8Yk+7CJgffHkiIpmraMUmOoybQ9GKTWm5fjSzUFoC7YFPzGxe5LE+wGVmdgLggOXA1UmpUEQkQw2fuZDZizYA8Gink1N+/WhmobwNWClfmh58OSIi4dGtdZN9/k61lC6lFxHJJs0b1EzLyHsvLaUXEQkpBbiISEgpwEVEQkoBLiISUgpwEZGQUoCLiISUAlxEJGCpWqGpABcRCdjeFZrDZy5M6nW0kEdEJGCpWqGpEbiISMD2rtBs3sAfk5CslooCXEQkyZLVUlELRUQkyZLVUlGAi4gkWbI2vVILRUQkpBTgIiIhpQAXEQkpBbiISEgpwEVEQkoBLiISUgpwEZGQUoCLiISUAlxEJJl++AHGjPF/B0wBLiISg6g3pvr+e7jvPmjUCDp3hmnTAq9FAS4iEoNyN6bauhWGDoWGDaFHD2jSBGbNgksvDbwW7YUiIhKDMjem2rwZ7r/fj7o3boTf/x5uuQV+/euk1VLuCNzM6pnZ62a2wMw+NbNukcdrmdkMM1sU+btm0qoUkayWqiPIgrD/Xt9s2AD9+rG7fn249VY2n3gyzJkDr7yS1PCG6Foou4CezrmjgFOA683saKA3MMs51xiYFflcRCRmqTqCLFDr1sHNN0NBAQweTFGTkzjnihF0bdcfWrRISQnltlCcc2uBtZGPt5rZAqAucAHQKvK0CcAbwN+TUqWIZLVUHUEWiDVr4K67YPRo2L4d2rWDvn2pUC2fvJkLU/o9mHMu+iebFQCzgWOAlc65GsW+tsk5V6KNYmadgc4A9evXb75ixYoESxYRSYMVK2DIEBg/Hvbsgfbt4R//gMaNk35pMytyzhXu/3jUs1DM7EDgGaC7c25LtK9zzo12zhU65wpr164d7ctERDLD4sXQqRP88pcwbhxccQUsXOiDPAXh/VOiCnAzq4QP7yecc1MiD68zs/zI1/OB9ckpUUQkNfa5mbpgAVx+ORxxBEycCNdcA0uWwKhRfopgBohmFooB44AFzrl7in1pGtAx8nFH4LngyxMRSZ3hMxey/p332fXHS6FpU5g61c/lXrbMTxGsVy/dJe4jmnngLYH2wCdmNi/yWB9gCDDZzDoBK4HgZ6mLiKRKUREjJt1GjRnT2X3ggb6/3aMH5OWlu7IyRTML5W3AyvjyGcGWIyKSYu++C7ffDi+9RI0aNWDAACp07Qo1M39pi1ZiikjucQ7efNMH92uv+VH2HXfAddfBQQelu7qoKcBFJHc4B6++CgMHwttvQ506MGyYv0FZrVq6q4uZNrMSkYwV2BJ75+D55+GUU6BNG1i+3N+UXLYMevYMZXiDAlxE0iSacE54if2ePfDMM3DiiXD++bB+vZ8GuHgxdOkCVarEWX1mUAtFRNJibzgDPNrp5FKfE/cS+927YdIkGDQIPvvMb+n6yCPw5z9DpUqJlJ1RFOAikhbRhPPenf+itnMnPPEEDB4Mixb5udwTJ0LbtlChQqIlZxy1UEQkLUpsy5qI7dv95lJHHAFXXul72k8/DR9/DJddlnB4Z+p2twpwEQmv//7X34z85S/h6quhdm1/s/KDD+CSS+BnwURcpm53qwAXkfDZts1P/2vYELp2ZWv+YdzZYzhFk16i6NiWdBj/XqCj5W6tm3Ba47yM2+5WPXARCUTRik0Mj+yHHUhbpDRbtsDIkXDvvf4knDPOgEmTuH7xAcxetIFPZy0CKPfmaDR17/94TL34FFGAi0jciodcNLNK4rZpEwwf7v9s3gznnAP9+sGppwLQrcCPtouPkKMdLZdVd1K/n4AowEUkbsVDLimn6nz9tR9tjxzpT3u/8EIf3M2b7/O0/UfIsQRuWXWH4ZSgmE7kSVRhYaGbO3duyq4nIsmVtLbJ2rW+x/3QQ/5GZdu20KcPHHdcIG+fknZPgMo6kUcjcBGJW+C94VWrYOhQGDMGdu3yC2/69IEjjwzuGoSjPRINBbiIpN+yZX43wEce8fuWXHEF9O4NjRol5XJhaI9EQ9MIRbJUpi4+2cfChT6sGzeGCRPgr3/1+5SMGZO08IaAFxGlkQJcJEtlyuKTUn+QfPqpb48cdZTfs6RLF1i6FB54ABo0SO61s4haKCJZKlPaBPv0m0+s7PfinjLFL3fv2dP/qVPnx+cHeYMxW3rdZVGAi2SpIG8wJhKq3Vo34fCln9L9yfvhr6/4E2/69YPu3eHnPy/x/CBDN1N+iCWLAlxEyhV3qL79Ns1vu43mM2ZArVr+CLMuXaBGjTJfEmToZuoKyqAowEWkXDGFqnP+nMnbb/fnTh5yCNx5J1x7LVSvXu7Lsz10g6SbmCJSrqhmbTgHL70ELVtC69Z+P+777vNTBG++Oarw/inZfkMyHgpwEUnMnj3w3HPQooXfo2TNGj+bZMkS6NYNqlYN5DKZMqsmkyjARaRU5Y54d++GyZOhWTO/R8mmTTB2LCxaRNE57ejwxEc5saVrOqkHLiKlKvPG5a5d8NRT/rzJzz/3y9wfewzatYOKFSOv/TDw6XvqjZekEbiIACVH3CVGvDt3wrhxPrDbt/eHA0+aBPPnw+WX/xjepb42A2RjD10jcBEBSo64fxzxbt8ODz4IQ4bAypV+K9epU+H888s8siwTR8vZuKin3BG4mY03s/VmNr/YYwPMbI2ZzYv8OSe5ZYpINBIZZZYYNX//vT9A4fDD4brroG5dmD4d3n/f97wDOm8yVTLxt4JERfNf4BGgTSmP3+ucOyHyZ3qwZYlIPBKZqfHjVMFaFeGuu/x5k927+42mZs6Ed96Bs88Gs7jrC6qNEc/7ZMsGVsWV20Jxzs02s4LklyIiiUpoFeO33/oT3u+9FzZuhDPPhFtugd/8JrD6gmpjZGM7JB6J9MC7mFkHYC7Q0zlX6o9CM+sMdAaoX79+ApcTkfLE1Xv+5hvfKhkxwof4eedB375wcvDBGNQy+Wzf4yRaUR2pFhmBv+CcOybyeR1gA+CA24F859xV5b2PjlQTySDr18Pdd/tFN9u2wcUX+02mmjVLd2Wyn0CPVHPOrSv2xmOAFxKoTURS6csvfY971Cg/w+RPf/Ij7qZN012ZxCiu28hmll/s04uA+WU9VyQXJGuOcaDvu2KFn03SsKHvdbdtCwsWwMSJCu+QKncEbmZPAq2APDNbDfQHWpnZCfgWynLg6iTWKJLxknVTLZD3XbzYz+GeMMHPINl73uThhwdWp6RHNLNQLivl4XFJqEUktBK9qVbWgQkJve/nn/vl7hMn+lWTV18Nf/871KsXV42SeaK6iRkU3cQUKV2HcXOYvWgDpzXOi3mkXSL8P/nEH1v2739DlSp+H+6ePSE/v/w3k4wU6E1MEQlW8ZF2rMeX7W2zFCz/nObzn4Vnn/V7b/fuDT16QO3ayS5f0iRca2FFQiDRVYKxrqbsU+tbnp0+mNsGXgFvvAH9+8Py5TB4cLnhHU+t2bgpVFgpwEXKEWtgJXrwQNR7drz5JrRuzZEXn8UJaxf6wF6+HAYM8OdPJqlWHayQOdRCESlHrDNBEr2h+ZOrKZ2DGTPYessAqr/3LjvzDqHSsGFwzTVQrVpU71+8RRNPrVoFmTl0E1MyVqy94Kyuwzl48UV/c3LOHDbWrM2IEy9i1SV/Zty1p8f0VoncMJX00E1MCZ1M2bAorXtb79njb0oOHAgffggFBTBqFMtbnc/St1bENQrWCDp7KMAlY+V00Oze7acBDhrkT7xp3Bgefhj+8heoVIkTgUeb/CKut87EwxYkPgpwyVg5GTQ7d/qFN4MHw8KFcPTR8MQTfr+SChXSXZ1kGAW4SCbYscMvdb/jDli2DI4/3o/AL744dCffSOro/wyRdPrhBxg5Eho1gs6dIS8Ppk3z/e4//lHhLT9J/3dIqIV2Ucl338E99/idAW+4ARo0gJdfhjlz/IEKCRxbJrlDLRQJtUyZqRK1LVv8AQp33w0bNsDvfgdPPgmnn67QlphpBC6hFpqTxjdtgn/+008D/Mc/oLAQ3n4bZs2CVq2iCu/Q/rYhSaMAl1BL50njUQXqhg3Qp49vkQwY4A8Ifu89eOklaNkyputpCbvsTwEuEqefDNSvvoJevXxwDxkCZ50F8+bBc8/BSScBsY+oQ/PbhqSMeuAicSp1odHq1TB0KIwZ46cGXnaZH4EffXSJ18fav8/JefHykxTgInHaJ1CXL/cj7Ycf9svfO3Tw+3E3blzm61O50jQj9nORwCnARRKxaJFffPPYY37O9lVX+WPLCgrKfWkqR9Shm60jUVGAi8Tjs8/8PiVPPQWVK8P118NNN0HduumurFQ5va9MFlOAi8Ri3jwf3M88A1Wr+rMme/aEOnXSXdlPUv88O2kWimStQOdNv/8+nH8+NGvG9y++xNrrb/R976FDMz68JXspwCVrFZ/mF3eYv/MOtGkDLVrA22/zzAWdOaXzOP5+wqV+3xKRNFILRbJW8b5vTDfxnPOHA99+O7z+uj8YeMgQuO46Cjbu4oTIbA6RdNORapIToppG5xy88ooP7v/8B/Lz/Y3Jzp2jPm9SJBl0pJrktHIPCn7+eb679Z9U++gDdhxal8r/+pefEnjAAaktVCQGCnDJXXv2+NkkgwbBRx+xrXZdbmtzA+svbMvDV/863dWJlKvcm5hmNt7M1pvZ/GKP1TKzGWa2KPK3lnZJeOza5Y8pO+YYaNvWH6rw6KOsfvdD1v7xL3Rp0zTdFYpEJZpZKI8AbfZ7rDcwyznXGJgV+Vwks+3c6Ze6H3UUXH45VKjA0vvH0rHneIpOO5fmjWr/uLOhtm6VMCg3wJ1zs4GN+z18ATAh8vEE4MKA6xIJzvbt8NBDfl+Sq66C6tVhyhT46CMGVDmGN5dsKrGjoLZulTCItwdexzm3FsA5t9bMDinriWbWGegMUL9+/TgvJxKH//7X7wo4dCisWQMnnwz/+hecc86PByiUtcRcS88lDKKaRmhmBcALzrljIp9vds7VKPb1Tc65cvvgmkYoKbFtmx9xDxsG69bBaadBv37QurWOLZNQKmsaYbwrMdeZWX7kjfOB9YkUJxKLMvvT337rZ5QUFPj528cdB2++6f+ceabCW7JOvAE+DegY+bgj8Fww5YiUr0R/euNG6N/fn37Trx+ccgq8+y68+qoffYtkqXJ74Gb2JNAKyDOz1UB/YAgw2cw6ASuBS5NZpEhxe/vSPU+o6Q8IHjnSt00uuojPOnVlyFdV6JbfhOZprlMk2coNcOfcZWV86YyAaxGJSvPKP/Dop5Pghof8HO62baFvXzj2WIaMm5OxBxfoVBwJmnYjlKQKdD71ypXQpQs0bAgjRsCll8KCBf5QhWOPBTL74F9NTZSgaSm9JFUgR3ktXeqPLZsQWXrQsaM/b7JRoxJPjebggnSNhDU1UYKmAJekSii0vvgCBg/2y94rVoS//c2fN5ngeoJUng+5/w+LTGvrSLgpwCVwCYfW/PkwcCBMnux3A+zaFXr1gkMPDaS+VI6EdZiwJJMCXAIXd2h9+KEP7ilT4MAD4eab4cYb4ZAyF/rGJZUjYbVNJJkU4BK4mENrzhx/iMKLL8LBB8Mtt0D37lCrVhKrTA21TSSZFOASuKhDa/ZsP+KeMcOH9cCBfpbJwQcnv0iRLKAAl9RyDmbO9GE9e7ZvjwwdCtde69smIhI1BbikhnMwfbpvlcyZ429I3nefn1lStWq6qxMJJS3kkeTaswemToXCQjj3XPjqK3jwQT+3u1s3hbdIAhTgOSZlJ83s3g2TJsEJJ8DFF8OWLTB+PCxaBNdcA//3f8m9vkgOUIBH5MoRWklfzr1rFzz2GDRtCu3a+c8ff9wveb/ySqhUKTnXFclBCvCIXNmnIml7hezYAWPHwhFHQIcOfoQ9ebJflPOXv/iVlGXIlR+eIkHTTcyIXFlwEfi85B9+gHHj4M47YdUq3+u+5x447zz4WXTjA61WFImPAjxCCy5i9P33MGoU3HUXrF0Lv/oVjB4NZ50V88k3xX94astVkegpwCU2W7fCAw/A3XfD119Dq1Z+s6lWreI+sqz4D88OGbyft0imUQ9cSii1J715M9x+O7vqN4Devfn2qGPhrbfg9dfht78N7LzJTN7PWyTTKMBzQKw3Cfe5obthgz9nskEDuPVWPm7QlAva380NHQZTVK9p4Dcf947G1T4RKZ9aKDkg1puE3Vo34aBvv6Hfe49Dt4fhu+/gkkugXz9czQYcHOlR6+ajSHopwLNU8ZuBMc2wWbOG5vcMpfno0X5qYLt20KePn9cNNOd/YZ0rM3dEMpU551J2scLCQjd37tyUXS+X7b0ZeFrjvKhGx5+88zHr+w7gt+++yM/27IH27f2J740bp6BaEfkpZlbknCvc/3H1wEOurP521DcDFy+GTp046vQT+c1bz/PGqX/wy93Hj1d4i2Q4tVBCrqw+dLnz2hcs8OdNTpwIlSuzsX0nBh19Lh3a/hp0A1EkFBTgccqUBScx96E//tjvxf3001ClCvToAb16ccgvfsHwJNYpIsFTgMcpU2ZgRL2CdO5cvxf3tGlQvbrvb/foAXl5yS9SRJJCPfA4hWbByX/+A2efDSed5E/AGTAAVqyAQYPSGt7awEokcQkFuJktN7NPzGyemeXU9JJMXHDyYygu3whvvAFnnAEtW/rR9x13+ODu3x9q1tz3+WkI0VzZ/VEkmYJoofzWObchgPcREuutD5/xBbz6KgfeeTUs+gh+8Qu/Z8nVV0O1aiWfn8Y2kOaQiyROPfBSpPMGZVyh6hy88AIPjhxAtY8+YEf+oXD//dCpk79RWYZ0hqh2fxRJXKIB7oBXzcwBo5xzo/d/gpl1BjoD1K9fP8HLpUZoRqZ7z5scOBDmzaNaQQGMGkXljh2jOrJMISoSbgmtxDSzQ51zX5rZIcAM4Abn3Oyynh+WlZiZMkWwTHvPmxw0CD77DJo08cvd//xnHVkmkoXKWomZ0AjcOfdl5O/1ZjYVaAGUGeBhkbEj0507/d7bgwf71ZJNm8KTT8Kll0KFCumuTkRSLO5ZKGZWzcyq7/0Y+D0wP6jC5H+zRD5Y+JU/7aZJE7jySr6vXIUR195B0fNv+s2mFN4iOSmREXgdYKr5jfwrAhOdcy8HUpUA8MD0j2k45Uka3DQVNq2HFi3g/vu55qs8Zi/+hrmvLebRTj9Pd5kikiZxB7hzbilwfIC1yF7btsFDD/HQnXdRacN6tp50CgtveICB2+vS7dgj6HYsYAs1BU8kx2klZibZssX3twsK4KabqHTcMfDGG1R/710G7jiM2Yu/YfjMhRm5iEhEUk8BHoBYVzSWeP6mTX6Je4MG0LcvnHyyXwI/axacfjoQoqX7IpIyWsgTgFjnje99fvWtm2i+fjaMHOlPe7/wQn/+ZPPmJV6TsTNjUiTjp3aKpEFOBHiy//HHuqKx53EH027yCM5661n44b9+GmDfvnDccYHXli0yZfdHkUySEwGe7H/8UY+OV62CoUM5fswYjt+50y+86dMHjjoq8JqyjfZOESkpJwI87f/4ly3zuwE+8ojft6RjR78fd6NG6aknTRL5TSjXW0gipcmJm5ipnLWxzw3KhQvhiiv82ZITJsBf/+rPoBw7NjThHeSWs9pCViRYOTECT6XhMxfy1X/msmP0rTB3pt9U6oYb4Kab4NBD011ezIJsP6X9NyGRLKMAD9KHHzL86YHUfPkFdletBr16Qc+ecMghP/myTJ5hEWToqg0iEiwFeBDee8+fN/nCC9Q86CDo148K3bvDz6Nb5p7JMywUuiKZSwGeiLfe8ntxv/oq1KoFt93m2yU1asT0NmotiEg8FOCxcg5ee82PuN9807dH7rwTrr3Wn/YeB41yRSQeCvBoOQcvv+yD+913/Q3J++6Dv/0NqlZNd3UikoNyYhphQvbsgWefhZNOgnPOYcMXy1hx+zBYsgS6dVN4i0jaKMDLsns3TJ4MzZrBRRfBpk2M7diHU696kFvyfw0HHFDuWwQ5h1pEZH8K8P3t2gWPPw7HHAN/+hNs3w6PPgpffEGzf/bi1CPzo77ZqIUrIpJM6oHvtWMHPPaYX/K+ZAkce6w/OPiSS348sizWm42aXSIiyaQA/+EHePhhGDIEVq70W7lOnQrnnw8/S+wXFM0uEZFkCnULJaEe8/ffs6r/HWzMrw/XXQd168L06fD++35f7gTDW0Qk2UKdUnH1mLduhaFDoWFD6t3Wh4XV6zCk5/3wzjtw9tngD2kWEcl4oW6hxNRj3rzZn3xz772wcSP8/vd8/rfuPPBtLf96BbeIhEzoAnz/jZ/K7TF/841fcDNihD80+Nxz/bFlJ5/MkcCjKalaRCR4oQvwqDd+WrcO7rkHHngAtm2Diy/2wd2sWYoqFRFJrtD1wMs9nf3LL6FHD2jYEIYNg/POg/nz4ZlnoFmzxE+QFxHJEKEbgZfZNlmxwm8qNW6cX0V5+eX+vMkm+wZ9vCfIR/t8EZFUCV2Al7BkiV98M2GCvxF55ZXQu7cfgZci1sU1WowTnUw+lEIkW5lzLmUXKywsdHPnzg3mzT7/HAYPhokToWJFvyvgzTdDvXrBvL/EpMO4OcxetIHTGufpNxWRgJlZkXOucP/HExqBm1kbYDhQARjrnBuSyPtF5ZNP/CEK//43VKnidwTs1Qvy85N+aSmbflMRSb24R+BmVgFYCJwJrAbeBy5zzn1W1msSGoEXFfngfvZZOPBA6NIFbrwRateO7/1EREIiGSPwFsBi59zSyAWeAi4AygzwuN14o1+AU6MG9O8PXbv6I8xERHJYIgFeF1hV7PPVQInmp5l1BjoD1K9fP74rnXEG5OXB9dfDwQfH9x4iIlkmkQAvbe15iX6Mc240MBp8CyWuK/3hD/6PiIj8KJGFPKuB4lM+DgO+TKwcERGJViIB/j7Q2MwamllloB0wLZiyYqcVkyKSa+IOcOfcLqAL8AqwAJjsnPs0qMJipePLRCTXJDQP3Dk3HZgeUC0J0TxkEck14V9KH6Hjy0Qk14RuN0IREfEU4CIiIaUAFxEJKQW4iEhIKcBFREJKAS4iElIKcBGRkApFgGuZvIhISaEI8FiXySvwRSQXhGIlZqzL5HWSvIjkglAEeKzL5LUviojkglAEeKy0L4qI5IJQ9MBFRKQkBbiISEgpwEVEQkoBLiISUgrwFNHcdBEJmgI8RXRmp4gELSunEWYizU0XkaApwFNEc9NFJGhqoYiIhJQCXEQkpBTgIiIhpQAXEQkpBbiISEgpwEVEQkoBLiISUuacS93FzL4GVsT58jxgQ4DlhIG+59yg7zk3JPI9N3DO1d7/wZQGeCLMbK5zrjDddaSSvufcoO85NyTje1YLRUQkpBTgIiIhFaYAH53uAtJA33Nu0PecGwL/nkPTAxcRkX2FaQQuIiLFKMBFREIqFAFuZm3M7AszW2xmvdNdT7KZWT0ze93MFpjZp2bWLd01pYKZVTCzD83shXTXkgpmVsPMnjazzyP/rU9Nd03JZmY9Iv9PzzezJ83sgHTXFDQzG29m681sfrHHapnZDDNbFPm7ZhDXyvgAN7MKwL+As4GjgcvM7Oj0VpV0u4CezrmjgFOA63PgewboBixIdxEpNBx42Tl3JHA8Wf69m1ldoCtQ6Jw7BqgAtEtvVUnxCNBmv8d6A7Occ42BWZHPE5bxAQ60ABY755Y653YATwEXpLmmpHLOrXXOfRD5eCv+H3bd9FaVXGZ2GPAHYGy6a0kFMzsIOA0YB+Cc2+Gc25zeqlKiIlDFzCoCVYEv01xP4Jxzs4GN+z18ATAh8vEE4MIgrhWGAK8LrCr2+WqyPMyKM7MCoBkwJ72VJN19wM3AnnQXkiKHA18DD0faRmPNrFq6i0om59waYBiwElgLfOucezW9VaVMHefcWvADNOCQIN40DAFupTyWE3MfzexA4Bmgu3NuS7rrSRYzOxdY75wrSnctKVQROBF40DnXDPiOgH6tzlSRvu8FQEPgUKCamV2e3qrCLQwBvhqoV+zzw8jCX7v2Z2aV8OH9hHNuSrrrSbKWwPlmthzfIvudmT2e3pKSbjWw2jm39zerp/GBns1aA8ucc18753YCU4BfpbmmVFlnZvkAkb/XB/GmYQjw94HGZtbQzCrjb3pMS3NNSWVmhu+NLnDO3ZPuepLNOfcP59xhzrkC/H/f15xzWT0yc859BawysyMiD50BfJbGklJhJXCKmVWN/D9+Bll+47aYaUDHyMcdgeeCeNOKQbxJMjnndplZF+AV/F3r8c65T9NcVrK1BNoDn5jZvMhjfZxz09NYkwTvBuCJyMBkKXBlmutJKufcHDN7GvgAP9PqQ7JwSb2ZPQm0AvLMbDXQHxgCTDazTvgfZJcGci0tpRcRCacwtFBERKQUCnARkZBSgIuIhJQCXEQkpBTgIiIhpQAXEQkpBUCVFW4AAAAKSURBVLiISEj9PxIUS5xsh6wnAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Imports\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Defining dataset\n",
    "# Data is generated by y = 2*x + 1 + 3*epsilon, where epsilon ~N(0;1)\n",
    "X = np.linspace(0, 10, 100)\n",
    "Y = 2*X + 1 + 2*np.random.randn(100)\n",
    "\n",
    "# Plotting data\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X, Y, s=2);\n",
    "ax.set_title = \"Fitting a line\"\n",
    "ax.set_xlabel = \"input data\"\n",
    "ax.set_ylabel = \"output\"\n",
    "\n",
    "# Calculation of line parameters\n",
    "a = (np.mean(X*Y) - np.mean(X)*np.mean(Y))/(np.mean(X**2) - np.mean(X)**2)\n",
    "b = (np.mean(Y)*np.mean(X**2) - np.mean(X) * np.mean(X*Y))/(np.mean(X**2) - np.mean(X)**2)\n",
    "print(f\"a = {a}, b = {b}\")\n",
    "\n",
    "# Plotting line of best fit into fig\n",
    "Y_hat = a*X + b\n",
    "ax.plot(X, Y_hat, c='r');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The most commmon metric for evaluating a linear regression model is $R^2$  \n",
    "$R^2 = 1 - \\frac{SS_{res}}{SS_{tot}}$  \n",
    "$SS_{res} = \\sum_{i=1}^{N}(y_i - \\hat{y_i})^2$  \n",
    "- $SS_{res}$ is what we have already seen in the cost function. It's the sum of squared differences between predicted values and actual values\n",
    "$SS_{tot} = \\sum_{i=1}^{N}(y_i - \\bar{y})^2$\n",
    "- $SS_{tot}$ returns the sum of squared differences between actual values and their mean\n",
    "- As we know, the simplest model one could fit is simply outputting the mean of targets regardless of the inputs. For this case $SS_{res}$ is equal to $SS_{tot}$, and $R^2$ then equals 0 meaning our model has very low predictive power.\n",
    "- If our model was somehow able to predict exactly the desired values, $SS_{res}$ would then be equal to 0, and $R^2$ would be equal to 1, which would be the theoretical ideal case.\n",
    "- In some extreme cases $R^2$ could even be negative if the predictions would actually be worse then simply predicting the mean.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 = 0.8928137339361116\n"
     ]
    }
   ],
   "source": [
    "# Using the same model parameters from the previous code cell\n",
    "# Calculate R^2\n",
    "\n",
    "SS_res = (Y_hat - Y).dot(Y_hat - Y)\n",
    "SS_tot = (Y - Y.mean()).dot(Y - Y.mean())\n",
    "R2 = 1 - SS_res/SS_tot\n",
    "\n",
    "print(f\"R^2 = {R2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple linear regression\n",
    "- Multiple linear regression differs from 1D regression in that it allows for multiple input values. We denote the dimensionality of inputs as $D$\n",
    "\n",
    "## Preliminaries\n",
    "- the $D$-dimensional dataset with $N$ datapoints is given by $data = \\{((x_{11}, \\dots, x_{1D}), y_1), \\dots, ((x_{N1}, \\dots, x_{ND}), y_N)\\}$\n",
    "- an invididual datapoint $i$ are denoted as a column vector $\\mathbf{x_i}$ with dimension $Dx1$\n",
    "- we collect the input variables into an $NxD$ matrix $\\mathbf{X} = [\\mathbf{x_1^T}, \\dots, \\mathbf{x_N^T}]$, where rows represent individual datapoints and columns represent the input values\n",
    "- likewise $\\mathbf{Y} = [y_1, \\dots, y_N]^T$  \n",
    "- and $\\mathbf{\\hat{Y}} = [\\hat{y_1}, \\dots, \\hat{y_n}]^T$  \n",
    "- we're trying to find a $D$-dimensional line satisfying the equations:  \n",
    "$w_0 + w_1x_{11} + \\dots + w_Dx_{1D} = \\hat{y_1}$  \n",
    "$\\vdots$  \n",
    "$w_0 + w_1x_{N1} + \\dots + w_Dx_{ND} = \\hat{y_N}$\n",
    "- we denote $\\mathbf{w} = [w_1, \\dots, w_D]^T$\n",
    "- to make further notation easier, we absorb the constant term $w_0$ into $\\mathbf{w}$ and $\\mathbf{x_i}$\n",
    "- $\\mathbf{w} = [w_0, w_1, \\dots, w_D]^T$ and $\\mathbf{x_i} = [1, x_{i1}, \\dots, x_{iD}]^T$\n",
    "- then we can simply write $\\mathbf{\\hat{Y}} = \\mathbf{Xw}$\n",
    "- and $\\hat{y_i} = \\mathbf{w^Tx_i}$\n",
    "- as in the one dimensional case, we seek to minimize $J = \\sum_{i=1}^N(y_i - \\hat{y_i})^2 = \\sum_{i=1}^N(y_i - \\mathbf{w^Tx_i})^2$ by setting $\\frac{\\partial J}{\\partial w_j} = 0$ for all $j \\in \\{0, \\dots, D\\}$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Closed form solution\n",
    "$\\frac{\\partial J}{\\partial w_j} = 0$  \n",
    "$\\equiv \\frac{\\partial}{\\partial w_j} \\sum_{i=1}^N(y_i - \\mathbf{w^Tx_i})^2 = 0$  \n",
    "$\\equiv \\frac{\\partial}{\\partial w_j} \\sum_{i=1}^N(y_i - (w_0x_{i1} + \\dots + w_Dx_{iD}))^2 = 0$  \n",
    "$\\equiv \\sum_{i=1}^N2(y_i - (w_0x_{i1} + \\dots + w_Dx_{iD}))(-x_{ij}) = 0$  \n",
    "$\\equiv \\sum_{i=1}^N2(y_i - \\mathbf{w^Tx_i})(-x_{ij}) = 0$  \n",
    "$\\equiv \\sum_{i=1}^N(y_i(-x_{ij}) - \\mathbf{w^Tx_i}(-x_{ij})) = 0$  \n",
    "$\\equiv \\sum_{i=1}^N\\mathbf{w^Tx_i}x_{ij} = \\sum_{i=1}^Ny_ix_{ij} $  \n",
    "$\\equiv \\mathbf{w^T}\\sum_{i=1}^N\\mathbf{x_i}x_{ij} = \\sum_{i=1}^Ny_ix_{ij} $  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in fact we have $D+1$ of these equations:  \n",
    "$\\mathbf{w^T}\\sum_{i=1}^N\\mathbf{x_i}x_{i0} = \\sum_{i=1}^Ny_ix_{i0}$  \n",
    "$\\vdots$  \n",
    "$\\mathbf{w^T}\\sum_{i=1}^N\\mathbf{x_i}x_{iD} = \\sum_{i=1}^Ny_ix_{iD}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all these equations can be encompassed as matrices:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathbf{w^T}(\\mathbf{X^TX}) = \\mathbf{Y^TX}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and now we solve for $\\mathbf{Y}$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$(\\mathbf{w^T}(\\mathbf{X^TX}))^T = (\\mathbf{Y^TX})^T$  \n",
    "$(\\mathbf{X^TX})^T(\\mathbf{w^T}^T) = (\\mathbf{X^T(Y^T)^T})$  \n",
    "$(\\mathbf{X^T(X^T)^T})\\mathbf{w} = (\\mathbf{X^TY})$  \n",
    "$(\\mathbf{X^TX})\\mathbf{w} = \\mathbf{X^TY}$  \n",
    "$(\\mathbf{X^TX})^{-1}(\\mathbf{X^TX})\\mathbf{w} = (\\mathbf{X^TX})^{-1}\\mathbf{X^TY}$  \n",
    "$\\mathbf{w} = (\\mathbf{X^TX})^{-1}\\mathbf{X^TY}$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we have obtained an equation to calculate $\\mathbf{w}$ directly\n",
    "- however numpy offers an optimized way to solve systems of linear equations with **np.linalg.solve()**\n",
    "- $\\mathbf{Ax = b} \\implies x = np.linalg.solve(A, b)$\n",
    "- thus we use the third from last equation $(\\mathbf{X^TX})\\mathbf{w} = \\mathbf{X^TY}$\n",
    "- $w = np.linalg.solve(X^TX, X^TY)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding multiple linear regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
