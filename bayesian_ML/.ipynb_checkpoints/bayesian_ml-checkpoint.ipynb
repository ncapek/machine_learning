{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the offline world it was sufficient to run experiments, ie. collect data, perform statistical tests and modify action based on that\n",
    "- Today in the online world things are moving too fast. Web behaviour must be responsive and there is too much data to handle manually. Updating actions based on data must happen automatically, hence the use of bayesian machine learning\n",
    "\n",
    "## Examples of A/B testing\n",
    "- Medicine\n",
    "    - A pharmaceutical company discovers a new drug for blood pressure and needs to find out if it works\n",
    "    - Then perform and experiment. Select 1 group to give the drug and a second group to give a placebo\n",
    "    - Measure the blood pressure of every person before and after their treatment and perform statistical tests to figure out if there is a difference between the groups\n",
    "    \n",
    "- Web pages\n",
    "    - A website wants to optimize the number of visitors who purchase something (conversion rate)\n",
    "    - Multiple versions of the website are used to test for trustworthiness of the design\n",
    "    - A test is performed to see if one version peform better than another\n",
    "\n",
    "## What is Bayesian machine learning?\n",
    "- Bayes rule is just basic probability\n",
    "- in the Bayesian approach, everything is a random variable\n",
    "- suppose we want to measure the height of students in a class\n",
    "    - frequentist approach:\n",
    "        - measure the height of everyone in the class using a normal distribution\n",
    "        - write down the likelihood of observing that data\n",
    "        - use maximum likelihood estimation to figure out the parameters (mean and variance)\n",
    "    - bayesian approach:\n",
    "        - in the bayesian approach the parameters are not scalar numbers\n",
    "        - they are probability distributions themselves\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability review\n",
    "- marginal distributions: $p(A), p(B)$\n",
    "- joint distribution: $p(A, B)$\n",
    "- conditional distributions: $p(A|B), p(B|A)$\n",
    "\n",
    "$p(A) = \\sum_{B} p(A,B)$<br>\n",
    "$p(B) = \\sum_{A} p(A,B)$<br><br>\n",
    "$p(A|B) = \\frac{p(A,B)}{p(B)} = \\frac{p(A,B)}{\\sum_{A} p(A,B)}$<br>\n",
    "$p(B|A) = \\frac{p(A,B)}{p(A)} = \\frac{p(A,B)}{\\sum_{B} p(A,B)}$<br>\n",
    "\n",
    "$p(B|A) = \\frac{p(A,B)}{p(A)} = \\frac{p(A|B)p(B))}{\\sum_{B} p(A|B)p(B)}$<br><br>\n",
    "\n",
    "- for continuous distributions, we call the probability distribution the probability density and instead of summations we integrate\n",
    "\n",
    "## Examples\n",
    "- suppose we want to find p(Buy|Country):<br>\n",
    "\n",
    "|  | CA | US | MX |\n",
    "|-|-|-|-|\n",
    "| Buy = True | 20 | 50 | 10 |\n",
    "| Buy = False | 300 | 500 | 200 | \n",
    "\n",
    "- an e-commerce website will likely be very interested if the website is performing poorly in a given country. The causes may be numerous: slower speed in given country due to distance from datacenters, irrevelant product placement etc.<br><br>\n",
    "\n",
    "marginal probabilities\n",
    "- p(country=CA) = 210/(210+550+320)=0.30\n",
    "- p(country=US) = 550/(210+550+320)=0.51\n",
    "- p(country=MX) = 210/(210+550+320)=0.19<br><br>\n",
    "\n",
    "joint probabilities:\n",
    "- p(Buy=True, Country=CA) = 20/1080 = 0.019\n",
    "- p(Buy=False, Country=CA) = 300/1080 = 0.28\n",
    "- p(Buy=True, Country=US) = 50/1080 = 0.046\n",
    "- p(Buy=False, Country=US) = 500/1080 = 0.46\n",
    "- p(Buy=True, Country=MX) = 10/1080 = 0.0093\n",
    "- p(Buy=False, Country=MX) = 200/1080 = 0.185<br><br>\n",
    "\n",
    "conditional probabilities:\n",
    "- p(Buy=True|Country=CA) = 0.019/0.30 = 0.07\n",
    "- p(Buy=False|Country=CA) = 0.28/0.30 = 0.93\n",
    "- p(Buy=True|Country=US) = 0.046/0.51 = 0.09\n",
    "- p(Buy=False|Country=US) = 0.46/0.51 = 0.91\n",
    "- p(Buy=True|Country=MX) = 0.0093/0.19 = 0.05\n",
    "- p(Buy=False|Country=MX) = 0.185/0.19 = 0.97<br><br>\n",
    "\n",
    "Independence:<br>\n",
    "$P(A,B) = P(A)P(B)$\n",
    "\n",
    "Example:\n",
    "- consider N succesive coin tosses\n",
    "- the value of every successive coin toss is independent of the previous one, since it doesn't effect the probability\n",
    "- we say the each coin toss is iid, independent and identically distributed\n",
    "- meaning the the value of coin toss k is completely independent of all previous coin tosses\n",
    "- the gambler's fallacy refers to the common thought process that if a person has lost many bets in a row, the probability of winning the next bet is somehow higher, since \"it should average over time\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum likelihood estimation\n",
    "- technique for statistical modelling\n",
    "- trying to fit a model to the data, such that the parameters model the data as closely as possible\n",
    "\n",
    "## Bernoulli distribution\n",
    "$p(x) = \\theta^x(1-\\theta)^{1-x}$<br>\n",
    "$data = \\{x_1, \\dots, x_n\\}$<br>\n",
    "$L(\\theta) = p(data|\\theta) = \\prod_{i=1}^N p(x_i|\\theta) = \\prod_{i=1}^N \\theta^x_i(1-\\theta)^{1-x_i}$<br>\n",
    "$log(L(\\theta)) = l(\\theta) = \\sum_{i=1}^N {x_ilog(\\theta) + \\sum_{i=1}^N (1-x_i)log(1-\\theta)}$\n",
    "- the goal is to find the value for $\\theta$ such taht the likelihood function is maximized<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{dl(\\theta)}{d\\theta} = \\frac{\\sum_{i=1}^N x_i}{\\theta} - \\frac{\\sum_{i=1}^N (1-x_i)}{1-\\theta} = 0$<br>\n",
    "$\\theta = \\frac{\\sum_{i=1}^N x_i}{N}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal distribution (TODO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability in code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07656372550983481"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "np.random.seed(0)\n",
    "\n",
    "mu = 170\n",
    "sd = 7\n",
    "\n",
    "# generate samples from distribution\n",
    "x = norm.rvs(loc=mu, scale=sd, size = 100)\n",
    "\n",
    "# maximum likelihood mean\n",
    "x.mean()\n",
    "\n",
    "# maximum likelihood variance\n",
    "x.var()\n",
    "\n",
    "# maximum likelihood standard deviation\n",
    "x.std()\n",
    "\n",
    "# unbiased variance\n",
    "x.var(ddof=1)\n",
    "\n",
    "# unbiased standard deviation\n",
    "x.std(ddof=1)\n",
    "\n",
    "# at what height are you in the 95th percentile?\n",
    "norm.ppf(0.95, loc=mu, scale=sd)\n",
    "\n",
    "# you are 160 tall, what percentile are you in?\n",
    "norm.cdf(160, loc=mu, scale=sd)\n",
    "\n",
    "# you are 180 tall, what is the probability that someone is taller than you?\n",
    "1 - norm.cdf(180, loc=mu, scale=sd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traditional A/B testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian A/B testing\n",
    "\n",
    "## Explore-exploit dilemma\n",
    "- exploitation = using the collected we already have in order make use of so far obtained data\n",
    "- exploration = making random choices in order to obatain more data which we can exploit later\n",
    "- greedy alrogithms are those that only exploit the data, which often leads to suboptimal results<br>\n",
    "\n",
    "This sections is devoted to algorithms which attempt to solve this problem:\n",
    "- epsilon-greedy\n",
    "- optimistic initial values\n",
    "- UCB1 (upper confidence bound)\n",
    "- Thompson sampling (bayesian bandit)\n",
    "\n",
    "All of these can be used in place of traditional A/B testing\n",
    "\n",
    "## Epsilon-greedy\n",
    "- insteady of always taking the greedy action, have some small probability of doing something random\n",
    "- the probability is given by epsilon\n",
    "- the reason for random actions is that we want to gather sufficient data about other possibilities\n",
    "- but if one bandit is optimal, we don't want to spend time on the suboptimals ones in the long run\n",
    "- to reduce this problem, we can select a decay schedule for epsilon\n",
    "- lower epsilons take longer to converge on average, but the long time return will be higher after convergence\n",
    "\n",
    "### Implementation trick\n",
    "- to figure out the average number of wins, we calculate the sample mean\n",
    "- to do so in constant time and space complexity, we calculate the mean incrementally<br>\n",
    "$\\bar{X}_N = \\frac{1}{N}((N-1)*\\bar{X}_{N-1} + X_N)$\n",
    "\n",
    "### Pseudocode\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# greedy\n",
    "while True:\n",
    "    j = argmax(predicted bandit means)\n",
    "    x = play bandit j and get reward\n",
    "    bandits[j].update_mean(x)\n",
    "    \n",
    "# epsilon greedy\n",
    "while True:\n",
    "    p = random number in [0; 1]\n",
    "    if p < epsilon:\n",
    "        j = choose random bandit\n",
    "    else:\n",
    "        j = argmax(predicted bandit means)\n",
    "    x = play bandit j and get reward\n",
    "    bandits[j].update_mean(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal j: 5\n",
      "mean estimate for bandit 0:  0.01282051282051282\n",
      "mean estimate for bandit 1:  0.06185567010309277\n",
      "mean estimate for bandit 2:  0.09333333333333334\n",
      "mean estimate for bandit 3:  0.14864864864864866\n",
      "mean estimate for bandit 4:  0.35211267605633806\n",
      "mean estimate for bandit 5:  0.7432587194169721\n",
      "total reward earned: 7189.0\n",
      "overall win rate: 0.7189\n",
      "num_times_explored: 468\n",
      "num_time_exploited: 9532\n",
      "num times selected optimal bandit: 9605\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "NUM_TRIALS = 10000\n",
    "EPS = 0.05\n",
    "BANDIT_PROBABILITIES = [0.01, 0.05, 0.1, 0.2, 0.5, 0.75]\n",
    "\n",
    "class Bandit:\n",
    "    def __init__(self, p):\n",
    "        # p = winrate\n",
    "        self.p = p\n",
    "        self.p_estimate = 0.5\n",
    "        self.N = 0\n",
    "        \n",
    "    def pull(self):\n",
    "        # simulate draw\n",
    "        return np.random.random() < self.p\n",
    "    \n",
    "    def update(self, x):\n",
    "        self.N += 1\n",
    "        self.p_estimate = ((self.N-1)*self.p_estimate + x)/self.N\n",
    "        \n",
    "def experiment():\n",
    "    bandits = [Bandit(p) for p in BANDIT_PROBABILITIES]\n",
    "    \n",
    "    rewards = np.zeros(NUM_TRIALS)\n",
    "    num_times_explored = 0\n",
    "    num_times_exploited = 0\n",
    "    num_optimal = 0\n",
    "    optimal_j = np.argmax([b.p for b in bandits])\n",
    "    print(\"optimal j:\", optimal_j)\n",
    "    \n",
    "    for i in range(NUM_TRIALS):\n",
    "        \n",
    "        # use epsilod-greedy to select the next bandit\n",
    "        if np.random.random() < EPS:\n",
    "            num_times_explored += 1\n",
    "            j = random.randint(0, len(bandits)-1)\n",
    "        else:\n",
    "            num_times_exploited += 1\n",
    "            j = np.argmax([b.p_estimate for b in bandits])\n",
    "            \n",
    "        if j == optimal_j:\n",
    "            num_optimal += 1\n",
    "            \n",
    "        # play bandit with largest sample rate\n",
    "        x = bandits[j].pull()\n",
    "        \n",
    "        # update rewards log\n",
    "        rewards[i] = x\n",
    "        \n",
    "        # update estimate\n",
    "        bandits[j].update(x)\n",
    "        \n",
    "    for idx, b in enumerate(bandits):\n",
    "        print(f\"mean estimate for bandit {idx}: \", b.p_estimate)\n",
    "        \n",
    "    # print total reward\n",
    "    print(\"total reward earned:\", rewards.sum())\n",
    "    print(\"overall win rate:\", rewards.sum() / NUM_TRIALS)\n",
    "    print(\"num_times_explored:\", num_times_explored)\n",
    "    print(\"num_time_exploited:\", num_times_exploited)\n",
    "    print(\"num times selected optimal bandit:\", num_optimal)\n",
    "    \n",
    "    # plot results\n",
    "    cumulative_rewards = np.cumsum(rewards)\n",
    "    win_rates = cumulative_rewards / (np.arange(NUM_TRIALS) + 1)\n",
    "    plt.plot(win_rates)\n",
    "    plt.plot(np.ones(NUM_TRIALS)*np.max(BANDIT_PROBABILITIES))\n",
    "    plt.show()    \n",
    "    \n",
    "experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimistic initial values\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
