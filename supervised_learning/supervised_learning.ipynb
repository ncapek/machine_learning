{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised learning\n",
    "**If you can't implement it, you don't understand it**\n",
    "\n",
    "- topic covered: naive bayes, k nearest neighbors, perceptron, decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "- in this course we use the MNIST dataset available here: https://www.kaggle.com/c/digit-recognizer/overview\n",
    "- it consists of images of handwritten digits\n",
    "- inputs are a flattened vector of 28x28 pixels, no RGB so values are 0-255\n",
    "- input will be scaled to 0-1\n",
    "- targets are digits from 0 to 9\n",
    "- 42 000 images in the training set, test set doesn't come with labels on kaggle, so we will only be using the training set\n",
    "\n",
    "<img src=\"./assets/mnist.png\"\n",
    "     alt=\"Markdown Monster icon\"\n",
    "     style=\"float: left;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest neighbors\n",
    "\n",
    "### Intuition\n",
    "- both simple conceptually and easy to implement in code  \n",
    "- sample problem:  \n",
    "    - Will you pass a course given that I know how many hours you studied?\n",
    "    - I have data about students from past semesters\n",
    "    - I can find students who have are \"closest to you\" in the number of hours they studied for their exam\n",
    "    - by finding the most similar students, I can estimate your performance based on their result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Name  | Hours studied  | Passed   |\n",
    "|-------|---|---|\n",
    "| Alice | 1 | N |\n",
    "| Bob   | 3 | N |\n",
    "| Carol | 6 | Y |\n",
    "| David | 7 | Y |\n",
    "| Eric  | 8 | Y |  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./assets/knn1.png\" alt=\"alt text\" width=\"400\" height=\"400\" margin-left=\"10\">     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- using 2-nearest neighbours yields (Alice, Bob) who both failed, so the the prediction for your result would also be fail\n",
    "- using 3-nearest neighbours yields (Alice, Bob, Carol) which now has 1 instance of pass, in this case we would still predict the majority results, ie. fail\n",
    "- another possibility is to weigh the results based on distance \n",
    "- or to have a heuristic to break ties\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concepts and implementation\n",
    "- given a $k$, find the $k$ nearest neighbours and use them to determine the prediction\n",
    "- finding the 1 nearest neighbour is simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x_0):\n",
    "    '''\n",
    "    Given a single instance of input data, output the prediction\n",
    "    '''\n",
    "    closest_distance = inf\n",
    "    closest_class = -1\n",
    "    for x, y in training_data:\n",
    "        d = dist(x, x_0)\n",
    "        if d < closest_distance:\n",
    "            closest_distance = d\n",
    "            closest_class = y\n",
    "    return closest_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- keeping track of an arbitrary number of closest neighbours however is not so simple  \n",
    "- for every datapoint $i$ I need to find their respective $k$ nearest neighbours\n",
    "example:\n",
    "    - $k = 3$ and I have stored distances [1, 2, 3]\n",
    "    - I see a point with distance 1.5, so I should replace the 3\n",
    "    - assuming we have $n$ datapoints in total, we need to look at all of them to make their respective prediction $\\implies O(n)$\n",
    "    - furthermore for every datapoint there is a list of $k$ closest neighbours, which needs to be iterated over to see if a datapoint should be updated $\\implies O(k)$\n",
    "    - in total then $\\implies O(kn)$\n",
    "    - improvements in complexity over the naive approach can be made by using a sorted list to hold the $k$ nearest neighbours\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading and transforming data...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sortedcontainers import SortedList\n",
    "from utils import get_data\n",
    "\n",
    "X, Y = get_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive bayes and bayes classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptrons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a web service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
